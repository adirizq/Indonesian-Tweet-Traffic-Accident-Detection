{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"!pip install sastrawi --quiet","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:03.629049Z","iopub.execute_input":"2022-12-02T04:13:03.629865Z","iopub.status.idle":"2022-12-02T04:13:16.888928Z","shell.execute_reply.started":"2022-12-02T04:13:03.629756Z","shell.execute_reply":"2022-12-02T04:13:16.887694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:16.892606Z","iopub.execute_input":"2022-12-02T04:13:16.892957Z","iopub.status.idle":"2022-12-02T04:13:16.900185Z","shell.execute_reply.started":"2022-12-02T04:13:16.892924Z","shell.execute_reply":"2022-12-02T04:13:16.899199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['PYTHONHASHSEED']=str(seed)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:16.901506Z","iopub.execute_input":"2022-12-02T04:13:16.902291Z","iopub.status.idle":"2022-12-02T04:13:16.909021Z","shell.execute_reply.started":"2022-12-02T04:13:16.902253Z","shell.execute_reply":"2022-12-02T04:13:16.907934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport transformers\nimport string\nimport re\nimport random\nimport keras\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tqdm import tqdm\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nfrom transformers import logging\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\nos.environ['PYTHONHASHSEED']=str(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:48:13.771845Z","iopub.execute_input":"2022-12-02T04:48:13.772251Z","iopub.status.idle":"2022-12-02T04:48:13.781391Z","shell.execute_reply.started":"2022-12-02T04:48:13.772216Z","shell.execute_reply":"2022-12-02T04:48:13.780201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/accident/twitter_label_manual.csv')\ndata = data[['full_text', 'is_accident']].rename(columns={'is_accident': 'label'})\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.564449Z","iopub.execute_input":"2022-12-02T04:13:22.565202Z","iopub.status.idle":"2022-12-02T04:13:22.636506Z","shell.execute_reply.started":"2022-12-02T04:13:22.565159Z","shell.execute_reply":"2022-12-02T04:13:22.635213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.64194Z","iopub.execute_input":"2022-12-02T04:13:22.642687Z","iopub.status.idle":"2022-12-02T04:13:22.664728Z","shell.execute_reply.started":"2022-12-02T04:13:22.642604Z","shell.execute_reply":"2022-12-02T04:13:22.663268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.668294Z","iopub.execute_input":"2022-12-02T04:13:22.668709Z","iopub.status.idle":"2022-12-02T04:13:22.684869Z","shell.execute_reply.started":"2022-12-02T04:13:22.668655Z","shell.execute_reply":"2022-12-02T04:13:22.683471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"pretrained_indobert = 'indolem/indobert-base-uncased'\npretrained_indobertweet = 'indolem/indobertweet-base-uncased'\npretrained_roberta_wiki = 'cahya/roberta-base-indonesian-522M'\npretrained_roberta_oscar = 'flax-community/indonesian-roberta-base'","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.685961Z","iopub.execute_input":"2022-12-02T04:13:22.686725Z","iopub.status.idle":"2022-12-02T04:13:22.692995Z","shell.execute_reply.started":"2022-12-02T04:13:22.686687Z","shell.execute_reply":"2022-12-02T04:13:22.691841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Data","metadata":{}},{"cell_type":"code","source":"stop_words = StopWordRemoverFactory().get_stop_words()\nlen(stop_words)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.694715Z","iopub.execute_input":"2022-12-02T04:13:22.695408Z","iopub.status.idle":"2022-12-02T04:13:22.70976Z","shell.execute_reply.started":"2022-12-02T04:13:22.695374Z","shell.execute_reply":"2022-12-02T04:13:22.708664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_tweet(text):\n        result = text.lower()\n        result = re.sub('\\n', ' ', result)\n        result = re.sub(r'@\\w+', '', result)\n        result = re.sub(r'http\\S+', '', result)\n        result = result.translate(str.maketrans('', '', string.punctuation))\n        result = re.sub(\"'\", '', result)\n        result = re.sub(r'\\d+', '', result)\n        result = ' '.join([word for word in result.split() if word not in stop_words])\n\n        return result.strip()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.713815Z","iopub.execute_input":"2022-12-02T04:13:22.714263Z","iopub.status.idle":"2022-12-02T04:13:22.727837Z","shell.execute_reply.started":"2022-12-02T04:13:22.714228Z","shell.execute_reply":"2022-12-02T04:13:22.726192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['full_text'].apply(lambda x: clean_tweet(x))\ndata = data.drop(columns=['full_text'])\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.729218Z","iopub.execute_input":"2022-12-02T04:13:22.729574Z","iopub.status.idle":"2022-12-02T04:13:22.823164Z","shell.execute_reply.started":"2022-12-02T04:13:22.729543Z","shell.execute_reply":"2022-12-02T04:13:22.822083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statistics import mean\n\nmax_len = mean(data.astype('str').applymap(lambda x: len(x)).max())\nprint(f'Average text length: {max_len}')","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.82685Z","iopub.execute_input":"2022-12-02T04:13:22.828885Z","iopub.status.idle":"2022-12-02T04:13:22.842137Z","shell.execute_reply.started":"2022-12-02T04:13:22.82885Z","shell.execute_reply":"2022-12-02T04:13:22.841223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_valid, test_data = train_test_split(data, test_size=0.2, random_state=seed)\ntrain_data, validation_data = train_test_split(train_valid, test_size=0.1, random_state=seed)\nprint(f'Train data size: {train_data.shape}')\nprint(f'Validation data size: {validation_data.shape}')\nprint(f'Test data size: {test_data.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.843673Z","iopub.execute_input":"2022-12-02T04:13:22.844347Z","iopub.status.idle":"2022-12-02T04:13:22.856217Z","shell.execute_reply.started":"2022-12-02T04:13:22.84431Z","shell.execute_reply":"2022-12-02T04:13:22.853738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"def encode(tokenizer, data) :\n    input_ids = []\n    attention_masks = []\n  \n    for text in data['text']:\n        encoded = tokenizer(text, max_length=128, padding='max_length')\n        \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids), np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.85945Z","iopub.execute_input":"2022-12-02T04:13:22.860519Z","iopub.status.idle":"2022-12-02T04:13:22.871807Z","shell.execute_reply.started":"2022-12-02T04:13:22.860483Z","shell.execute_reply":"2022-12-02T04:13:22.870753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Related Function","metadata":{}},{"cell_type":"code","source":"def save_history(history, lr, batch, model):\n    history_df = pd.DataFrame(history.history)\n    \n    path = f'results/history'\n    \n    if not os.path.exists(path):\n        os.makedirs(path)\n    \n    history_df.to_csv(f'{path}/{model}_batch={batch}_lr={lr}.csv')\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.87366Z","iopub.execute_input":"2022-12-02T04:13:22.874645Z","iopub.status.idle":"2022-12-02T04:13:22.885164Z","shell.execute_reply.started":"2022-12-02T04:13:22.87461Z","shell.execute_reply":"2022-12-02T04:13:22.884162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_graph(history, lr, batch, model):\n    \n    path = f'results/graph/{model}_batch={batch}_lr={lr}'\n    \n    if not os.path.exists(path):\n        os.makedirs(path)\n    \n    plt.plot(range(1, len(history.history['accuracy'])+1), history.history['accuracy'], label='Train')\n    plt.plot(range(1, len(history.history['val_accuracy'])+1),history.history['val_accuracy'], label='Validation')\n    plt.title(f'{model} Accuracy Curves\\nBatch Size = {batch}, Learning Rate = {lr}')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.savefig(f'{path}/{model}_batch={batch}_lr={lr}_accuracy.png', facecolor='white', dpi=300)\n    plt.close()\n    \n    plt.plot(range(1, len(history.history['loss'])+1), history.history['loss'], label='Train')\n    plt.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'], label='Validation')\n    plt.title(f'{model} Loss Curves\\nBatch Size = {batch}, Learning Rate = {lr}')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.savefig(f'{path}/{model}_batch={batch}_lr={lr}_loss.png', facecolor='white', dpi=300)\n    plt.close()\n    \n    plt.plot(range(1, len(history.history['f1'])+1), history.history['f1'], label='Train')\n    plt.plot(range(1, len(history.history['val_f1'])+1), history.history['val_f1'], label='Validation')\n    plt.title(f'{model} F1-Score Curves\\nBatch Size = {batch}, Learning Rate = {lr}')\n    plt.ylabel('F1-Score')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.savefig(f'{path}/{model}_batch={batch}_lr={lr}_f1.png', facecolor='white', dpi=300)\n    plt.close()\n    \n    plt.plot(range(1, len(history.history['precision'])+1), history.history['precision'], label='Train')\n    plt.plot(range(1, len(history.history['val_precision'])+1), history.history['val_precision'], label='Validation')\n    plt.title(f'{model} Precision Curves\\nBatch Size = {batch}, Learning Rate = {lr}')\n    plt.ylabel('Precision')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.savefig(f'{path}/{model}_batch={batch}_lr={lr}_precision.png', facecolor='white', dpi=300)\n    plt.close()\n    \n    plt.plot(range(1, len(history.history['recall'])+1), history.history['recall'], label='Train')\n    plt.plot(range(1, len(history.history['val_recall'])+1), history.history['val_recall'], label='Validation')\n    plt.title(f'{model} Recall Curves\\nBatch Size = {batch}, Learning Rate = {lr}')\n    plt.ylabel('Recall')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.savefig(f'{path}/{model}_batch={batch}_lr={lr}_recall.png', facecolor='white', dpi=300)\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.887239Z","iopub.execute_input":"2022-12-02T04:13:22.887905Z","iopub.status.idle":"2022-12-02T04:13:22.914037Z","shell.execute_reply.started":"2022-12-02T04:13:22.887796Z","shell.execute_reply":"2022-12-02T04:13:22.913186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2*((p*r)/(p+r+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.918022Z","iopub.execute_input":"2022-12-02T04:13:22.920851Z","iopub.status.idle":"2022-12-02T04:13:22.931982Z","shell.execute_reply.started":"2022-12-02T04:13:22.920815Z","shell.execute_reply":"2022-12-02T04:13:22.930972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IndoBERT","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\nindobert_tokenizer  = BertTokenizer.from_pretrained(pretrained_indobert)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:45:26.780281Z","iopub.execute_input":"2022-12-02T04:45:26.780724Z","iopub.status.idle":"2022-12-02T04:45:37.955062Z","shell.execute_reply.started":"2022-12-02T04:45:26.780691Z","shell.execute_reply":"2022-12-02T04:45:37.954134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_ids, train_attention_masks = encode(indobert_tokenizer, train_data)\nvalidation_input_ids, validation_attention_masks = encode(indobert_tokenizer, validation_data)\ntest_input_ids, test_attention_masks = encode(indobert_tokenizer, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:45:37.957136Z","iopub.execute_input":"2022-12-02T04:45:37.9576Z","iopub.status.idle":"2022-12-02T04:45:38.990675Z","shell.execute_reply.started":"2022-12-02T04:45:37.957562Z","shell.execute_reply":"2022-12-02T04:45:38.989743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def indobert(model, dropout, learning_rate):\n    input_ids = tf.keras.Input(shape=(128,), dtype='int32')\n    attention_masks = tf.keras.Input(shape=(128,), dtype='int32')\n    \n    output = model([input_ids,attention_masks])\n    output = output[1]\n    \n    output = tf.keras.layers.Dense(32, activation='relu')(output)\n    output = tf.keras.layers.Dropout(dropout)(output)\n\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs=[input_ids,attention_masks], outputs=output)\n    \n    model.compile(\n        Adam(learning_rate=learning_rate), \n        loss='binary_crossentropy', \n        metrics=['accuracy', f1, precision, recall])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:45:38.995714Z","iopub.execute_input":"2022-12-02T04:45:38.998301Z","iopub.status.idle":"2022-12-02T04:45:39.011034Z","shell.execute_reply.started":"2022-12-02T04:45:38.998258Z","shell.execute_reply":"2022-12-02T04:45:39.009859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertModel\n\nindobert_pretrained_model = TFBertModel.from_pretrained(pretrained_indobert, from_pt=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:45:39.019464Z","iopub.execute_input":"2022-12-02T04:45:39.022966Z","iopub.status.idle":"2022-12-02T04:46:23.722186Z","shell.execute_reply.started":"2022-12-02T04:45:39.022932Z","shell.execute_reply":"2022-12-02T04:46:23.721198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indobert_model = indobert(indobert_pretrained_model, dropout=0.1, learning_rate=5e-5)\nindobert_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:46:23.723732Z","iopub.execute_input":"2022-12-02T04:46:23.724081Z","iopub.status.idle":"2022-12-02T04:46:27.970531Z","shell.execute_reply.started":"2022-12-02T04:46:23.724045Z","shell.execute_reply":"2022-12-02T04:46:27.969355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\npath = 'illustration'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n    \nplot_model(indobert_model, to_file=f'{path}/IndoBERT.png', expand_nested=True, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:46:27.972247Z","iopub.execute_input":"2022-12-02T04:46:27.972622Z","iopub.status.idle":"2022-12-02T04:46:28.251287Z","shell.execute_reply.started":"2022-12-02T04:46:27.972586Z","shell.execute_reply":"2022-12-02T04:46:28.250133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(indobert_pretrained_model)\ndel(indobert_model)\nkeras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:46:28.253356Z","iopub.execute_input":"2022-12-02T04:46:28.254494Z","iopub.status.idle":"2022-12-02T04:46:28.273467Z","shell.execute_reply.started":"2022-12-02T04:46:28.254445Z","shell.execute_reply":"2022-12-02T04:46:28.2726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets = train_data['label'].values\nvalidation_targets = validation_data['label'].values\ntest_targets = test_data['label'].values","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:46:28.27904Z","iopub.execute_input":"2022-12-02T04:46:28.279323Z","iopub.status.idle":"2022-12-02T04:46:28.28443Z","shell.execute_reply.started":"2022-12-02T04:46:28.279298Z","shell.execute_reply":"2022-12-02T04:46:28.283469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learing_rate = [5e-5, 3e-5, 2e-5]\nbatch = [16, 32]\n\nbatch_for_test_result = []\nlearing_rate_for_test_result = []\ntest_loss = []\ntest_accuracy = []\ntest_f1 = []\ntest_precision = []\ntest_recall = []\n\nearly_stopping = EarlyStopping(monitor='val_f1', patience=3, verbose=0, mode='max')\n\nif not os.path.exists('results/models'):\n    os.makedirs('results/models')\n\nfor b in batch:   \n    for lr in learing_rate:\n        print(f'Training IndoBERT Model (Batch Size = {b}, Learning Rate = {lr})')\n\n        indobert_pretrained_model = TFBertModel.from_pretrained(pretrained_indobert, from_pt=True)\n        indobert_model = indobert(indobert_pretrained_model, dropout=0.1, learning_rate=lr)\n        \n        save_best = ModelCheckpoint(\n            f'results/models/indobert_batch={b}_lr={lr}.hdf5', \n            save_best_only=True, \n            save_weights_only=True,\n            monitor='val_f1', \n            mode='max'\n        )\n\n        history = indobert_model.fit(\n            [train_input_ids,train_attention_masks], \n            train_targets, \n            validation_data=([validation_input_ids, validation_attention_masks], validation_targets),\n            callbacks=[early_stopping, save_best],\n            epochs=10, \n            batch_size=b,\n            verbose=1\n        )\n\n        save_graph(history, lr, b, 'IndoBERT')\n        save_history(history, lr, b, 'IndoBERT')\n\n        print(f'Testing IndoBERT Model (Batch Size = {b}, Learning Rate = {lr})')\n        \n        indobert_model.load_weights(f'results/models/indobert_batch={b}_lr={lr}.hdf5')\n        \n        evaluation = indobert_model.evaluate(\n            [test_input_ids, test_attention_masks], \n            test_targets, \n            batch_size=b,\n            verbose=1\n        )\n\n        batch_for_test_result.append(b)\n        learing_rate_for_test_result.append(lr)\n        test_loss.append(evaluation[0])\n        test_accuracy.append(evaluation[1])\n        test_f1.append(evaluation[2])\n        test_precision.append(evaluation[3])\n        test_recall.append(evaluation[4])\n\n        del(indobert_pretrained_model)\n        del(indobert_model)\n        del(history)\n        del(evaluation)\n        keras.backend.clear_session()\n\n\ntest_result_df = pd.DataFrame({\n    'Training Batch Size': batch_for_test_result,\n    'Training Learning Rate': learing_rate_for_test_result, \n    'Test Loss': test_loss, \n    'Test Accuracy': test_accuracy,\n    'Test F1-Score': test_f1,\n    'Test Precision': test_precision,\n    'Test Recall': test_recall,\n})\n\npath = f'results/evaluation'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n\ntest_result_df.to_csv(f'{path}/IndoBERT.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:48:24.499174Z","iopub.execute_input":"2022-12-02T04:48:24.499789Z","iopub.status.idle":"2022-12-02T05:00:19.717212Z","shell.execute_reply.started":"2022-12-02T04:48:24.499752Z","shell.execute_reply":"2022-12-02T05:00:19.715943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IndoBERTweet","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\nindobertweet_tokenizer  = BertTokenizer.from_pretrained(pretrained_indobertweet)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.849852Z","iopub.execute_input":"2022-12-01T16:36:53.850371Z","iopub.status.idle":"2022-12-01T16:36:53.856307Z","shell.execute_reply.started":"2022-12-01T16:36:53.850329Z","shell.execute_reply":"2022-12-01T16:36:53.855144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_ids, train_attention_masks = encode(indobertweet_tokenizer, train_data)\nvalidation_input_ids, validation_attention_masks = encode(indobertweet_tokenizer, validation_data)\ntest_input_ids, test_attention_masks = encode(indobertweet_tokenizer, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.867966Z","iopub.execute_input":"2022-12-01T16:36:53.86838Z","iopub.status.idle":"2022-12-01T16:36:53.876287Z","shell.execute_reply.started":"2022-12-01T16:36:53.868336Z","shell.execute_reply":"2022-12-01T16:36:53.87506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def indobertweet(model, dropout, learning_rate):\n    input_ids = tf.keras.Input(shape=(128,), dtype='int32')\n    attention_masks = tf.keras.Input(shape=(128,), dtype='int32')\n    \n    output = model([input_ids,attention_masks])\n    output = output[1]\n    \n    output = tf.keras.layers.Dense(32, activation='relu')(output)\n    output = tf.keras.layers.Dropout(dropout)(output)\n\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs=[input_ids,attention_masks], outputs=output)\n    \n    model.compile(\n        Adam(learning_rate=learning_rate), \n        loss='binary_crossentropy', \n        metrics=['accuracy', f1, precision, recall]\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.877979Z","iopub.execute_input":"2022-12-01T16:36:53.87857Z","iopub.status.idle":"2022-12-01T16:36:53.886503Z","shell.execute_reply.started":"2022-12-01T16:36:53.87853Z","shell.execute_reply":"2022-12-01T16:36:53.885451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertModel\n\nindobertweet_pretrained_model = TFBertModel.from_pretrained(pretrained_indobertweet, from_pt=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.893198Z","iopub.execute_input":"2022-12-01T16:36:53.893508Z","iopub.status.idle":"2022-12-01T16:36:53.899089Z","shell.execute_reply.started":"2022-12-01T16:36:53.893481Z","shell.execute_reply":"2022-12-01T16:36:53.897969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indobertweet_model = indobertweet(indobertweet_pretrained_model, dropout=0.1, learning_rate=5e-5)\nindobertweet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.900794Z","iopub.execute_input":"2022-12-01T16:36:53.901676Z","iopub.status.idle":"2022-12-01T16:36:53.907481Z","shell.execute_reply.started":"2022-12-01T16:36:53.901632Z","shell.execute_reply":"2022-12-01T16:36:53.906344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\npath = 'illustration'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n    \nplot_model(indobertweet_model, to_file=f'{path}/IndoBERTweet.png', expand_nested=True, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.909128Z","iopub.execute_input":"2022-12-01T16:36:53.909977Z","iopub.status.idle":"2022-12-01T16:36:53.917395Z","shell.execute_reply.started":"2022-12-01T16:36:53.909937Z","shell.execute_reply":"2022-12-01T16:36:53.916633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(indobertweet_pretrained_model)\ndel(indobertweet_model)\nkeras.backend.clear_session()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets = train_data['label'].values\nvalidation_targets = validation_data['label'].values\ntest_targets = test_data['label'].values","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.918603Z","iopub.execute_input":"2022-12-01T16:36:53.919821Z","iopub.status.idle":"2022-12-01T16:36:53.927427Z","shell.execute_reply.started":"2022-12-01T16:36:53.919776Z","shell.execute_reply":"2022-12-01T16:36:53.926348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learing_rate = [5e-5, 3e-5, 2e-5]\nbatch = [16, 32]\n\nbatch_for_test_result = []\nlearing_rate_for_test_result = []\ntest_loss = []\ntest_accuracy = []\ntest_f1 = []\ntest_precision = []\ntest_recall = []\n\nearly_stopping = EarlyStopping(monitor='val_f1', patience=3, verbose=0, mode='max')\n\nif not os.path.exists('results/models'):\n    os.makedirs('results/models')\n\nfor b in batch:\n    for lr in learing_rate:\n        print(f'Training IndoBERTweet Model (Batch Size = {b}, Learning Rate = {lr})')\n\n        indobertweet_pretrained_model = TFBertModel.from_pretrained(pretrained_indobertweet, from_pt=True)\n        indobertweet_model = indobertweet(indobertweet_pretrained_model, dropout=0.1, learning_rate=lr)\n        \n        save_best = ModelCheckpoint(\n            f'results/models/indobertweet_batch={b}_lr={lr}.hdf5', \n            save_best_only=True, \n            save_weights_only=True,\n            monitor='val_f1', \n            mode='max'\n        )\n\n        history = indobertweet_model.fit(\n            [train_input_ids,train_attention_masks], \n            train_targets, \n            validation_data=([validation_input_ids, validation_attention_masks], validation_targets),\n            callbacks=[early_stopping, save_best],\n            epochs=10, \n            batch_size=b,\n            verbose=1\n        )\n\n        save_graph(history, lr, b, 'IndoBERTweet')\n        save_history(history, lr, b, 'IndoBERTweet')\n\n        print(f'Testing IndoBERTweet Model (Batch Size = {b}, Learning Rate = {lr})')\n        \n        indobertweet_model.load_weights(f'results/models/indobertweet_batch={b}_lr={lr}.hdf5')\n        \n        evaluation = indobertweet_model.evaluate(\n            [test_input_ids, test_attention_masks], \n            test_targets, \n            batch_size=b,\n            verbose=1\n        )\n        \n        batch_for_test_result.append(b)\n        learing_rate_for_test_result.append(lr)\n        test_loss.append(evaluation[0])\n        test_accuracy.append(evaluation[1])\n        test_f1.append(evaluation[2])\n        test_precision.append(evaluation[3])\n        test_recall.append(evaluation[4])\n\n        del(indobertweet_pretrained_model)\n        del(indobertweet_model)\n        del(history)\n        del(evaluation)\n        keras.backend.clear_session()\n\n\ntest_result_df = pd.DataFrame({\n    'Training Batch Size': batch_for_test_result,\n    'Training Learning Rate': learing_rate_for_test_result, \n    'Test Loss': test_loss, \n    'Test Accuracy': test_accuracy,\n    'Test F1-Score': test_f1,\n    'Test Precision': test_precision,\n    'Test Recall': test_recall,\n})\n\npath = f'results/evaluation'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n\ntest_result_df.to_csv(f'{path}/IndoBERTweet.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T16:36:53.929071Z","iopub.execute_input":"2022-12-01T16:36:53.929948Z","iopub.status.idle":"2022-12-01T16:36:53.936858Z","shell.execute_reply.started":"2022-12-01T16:36:53.9299Z","shell.execute_reply":"2022-12-01T16:36:53.936074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RoBERTa Wiki","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\nroberta_wiki_tokenizer  = RobertaTokenizer.from_pretrained(pretrained_roberta_wiki)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:22.935962Z","iopub.execute_input":"2022-12-02T04:13:22.938739Z","iopub.status.idle":"2022-12-02T04:13:36.933296Z","shell.execute_reply.started":"2022-12-02T04:13:22.938704Z","shell.execute_reply":"2022-12-02T04:13:36.932319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_ids, train_attention_masks = encode(roberta_wiki_tokenizer, train_data)\nvalidation_input_ids, validation_attention_masks = encode(roberta_wiki_tokenizer, validation_data)\ntest_input_ids, test_attention_masks = encode(roberta_wiki_tokenizer, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:36.937783Z","iopub.execute_input":"2022-12-02T04:13:36.940198Z","iopub.status.idle":"2022-12-02T04:13:37.763132Z","shell.execute_reply.started":"2022-12-02T04:13:36.940136Z","shell.execute_reply":"2022-12-02T04:13:37.762102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def robertawiki(model, dropout, learning_rate):\n    input_ids = tf.keras.Input(shape=(128,), dtype='int32')\n    attention_masks = tf.keras.Input(shape=(128,), dtype='int32')\n    \n    output = model([input_ids,attention_masks])\n    output = output[1]\n    \n    output = tf.keras.layers.Dense(32, activation='relu')(output)\n    output = tf.keras.layers.Dropout(dropout)(output)\n\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs=[input_ids,attention_masks], outputs=output)\n    \n    model.compile(\n        Adam(learning_rate=learning_rate), \n        loss='binary_crossentropy', \n        metrics=['accuracy', f1, precision, recall]\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:37.768227Z","iopub.execute_input":"2022-12-02T04:13:37.770541Z","iopub.status.idle":"2022-12-02T04:13:37.78268Z","shell.execute_reply.started":"2022-12-02T04:13:37.7705Z","shell.execute_reply":"2022-12-02T04:13:37.781761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFRobertaModel\n\nrobertawiki_pretrained_model = TFRobertaModel.from_pretrained(pretrained_roberta_wiki)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:13:37.787393Z","iopub.execute_input":"2022-12-02T04:13:37.790317Z","iopub.status.idle":"2022-12-02T04:14:53.976065Z","shell.execute_reply.started":"2022-12-02T04:13:37.790277Z","shell.execute_reply":"2022-12-02T04:14:53.974976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"robertawiki_model = robertawiki(robertawiki_pretrained_model, dropout=0.1, learning_rate=5e-5)\nrobertawiki_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:14:53.977628Z","iopub.execute_input":"2022-12-02T04:14:53.978024Z","iopub.status.idle":"2022-12-02T04:15:01.223244Z","shell.execute_reply.started":"2022-12-02T04:14:53.977987Z","shell.execute_reply":"2022-12-02T04:15:01.222191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\npath = 'illustration'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n    \nplot_model(robertawiki_model, to_file=f'{path}/Indonesian RoBERTa Wiki.png', expand_nested=True, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:15:01.226121Z","iopub.execute_input":"2022-12-02T04:15:01.226552Z","iopub.status.idle":"2022-12-02T04:15:02.316928Z","shell.execute_reply.started":"2022-12-02T04:15:01.226523Z","shell.execute_reply":"2022-12-02T04:15:02.315843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(robertawiki_pretrained_model)\ndel(robertawiki_model)\nkeras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:15:02.318715Z","iopub.execute_input":"2022-12-02T04:15:02.32045Z","iopub.status.idle":"2022-12-02T04:15:02.332751Z","shell.execute_reply.started":"2022-12-02T04:15:02.320402Z","shell.execute_reply":"2022-12-02T04:15:02.331612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets = train_data['label'].values\nvalidation_targets = validation_data['label'].values\ntest_targets = test_data['label'].values","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:15:02.334549Z","iopub.execute_input":"2022-12-02T04:15:02.335195Z","iopub.status.idle":"2022-12-02T04:15:02.341289Z","shell.execute_reply.started":"2022-12-02T04:15:02.335141Z","shell.execute_reply":"2022-12-02T04:15:02.340169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learing_rate = [1e-5, 2e-5, 3e-5]\nbatch = [16, 32]\n\nbatch_for_test_result = []\nlearing_rate_for_test_result = []\ntest_loss = []\ntest_accuracy = []\ntest_f1 = []\ntest_precision = []\ntest_recall = []\n\nearly_stopping = EarlyStopping(monitor='val_f1', patience=3, verbose=0, mode='max')\n\nif not os.path.exists('results/models'):\n    os.makedirs('results/models')\n\nfor b in batch:\n    for lr in learing_rate:\n        print(f'Training RoBERTa Wiki Model (Batch Size = {b}, Learning Rate = {lr})')\n\n        robertawiki_pretrained_model = TFRobertaModel.from_pretrained(pretrained_roberta_wiki)\n        robertawiki_model = robertawiki(robertawiki_pretrained_model, dropout=0.1, learning_rate=lr)\n        \n        save_best = ModelCheckpoint(\n            f'results/models/robertawiki_batch={b}_lr={lr}.hdf5', \n            save_best_only=True, \n            save_weights_only=True,\n            monitor='val_f1', \n            mode='max'\n        )\n\n        history = robertawiki_model.fit(\n            [train_input_ids,train_attention_masks], \n            train_targets, \n            validation_data=([validation_input_ids, validation_attention_masks], validation_targets),\n            callbacks=[early_stopping, save_best],\n            epochs=4, \n            batch_size=b,\n            verbose=1\n        )\n\n        save_graph(history, lr, b, 'RoBERTa Wiki')\n        save_history(history, lr, b, 'RoBERTa Wiki')\n\n        print(f'Testing RoBERTa Wiki Model (Batch Size = {b}, Learning Rate = {lr})')\n        \n        robertawiki_model.load_weights(f'results/models/robertawiki_batch={b}_lr={lr}.hdf5')\n        \n        evaluation = robertawiki_model.evaluate(\n            [test_input_ids, test_attention_masks], \n            test_targets, \n            batch_size=b,\n            verbose=1\n        )\n        \n        batch_for_test_result.append(b)\n        learing_rate_for_test_result.append(lr)\n        test_loss.append(evaluation[0])\n        test_accuracy.append(evaluation[1])\n        test_f1.append(evaluation[2])\n        test_precision.append(evaluation[3])\n        test_recall.append(evaluation[4])\n\n        del(robertawiki_pretrained_model)\n        del(robertawiki_model)\n        del(history)\n        del(evaluation)\n        keras.backend.clear_session()\n\n\ntest_result_df = pd.DataFrame({\n    'Training Batch Size': batch_for_test_result,\n    'Training Learning Rate': learing_rate_for_test_result, \n    'Test Loss': test_loss, \n    'Test Accuracy': test_accuracy,\n    'Test F1-Score': test_f1,\n    'Test Precision': test_precision,\n    'Test Recall': test_recall,\n})\n\npath = f'results/evaluation'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n\ntest_result_df.to_csv(f'{path}/RoBERTa Wiki.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:15:02.343305Z","iopub.execute_input":"2022-12-02T04:15:02.343663Z","iopub.status.idle":"2022-12-02T04:16:39.663614Z","shell.execute_reply.started":"2022-12-02T04:15:02.343629Z","shell.execute_reply":"2022-12-02T04:16:39.661363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RoBERTa OSCAR","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\nroberta_oscar_tokenizer  = RobertaTokenizer.from_pretrained(pretrained_roberta_oscar)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:16:45.01704Z","iopub.execute_input":"2022-12-02T04:16:45.01776Z","iopub.status.idle":"2022-12-02T04:16:56.3569Z","shell.execute_reply.started":"2022-12-02T04:16:45.017721Z","shell.execute_reply":"2022-12-02T04:16:56.355706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_ids, train_attention_masks = encode(roberta_oscar_tokenizer, train_data)\nvalidation_input_ids, validation_attention_masks = encode(roberta_oscar_tokenizer, validation_data)\ntest_input_ids, test_attention_masks = encode(roberta_oscar_tokenizer, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:16:56.359017Z","iopub.execute_input":"2022-12-02T04:16:56.359707Z","iopub.status.idle":"2022-12-02T04:16:56.975844Z","shell.execute_reply.started":"2022-12-02T04:16:56.359664Z","shell.execute_reply":"2022-12-02T04:16:56.974796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def robertaoscar(model, dropout, learning_rate):\n    input_ids = tf.keras.Input(shape=(128,), dtype='int32')\n    attention_masks = tf.keras.Input(shape=(128,), dtype='int32')\n    \n    output = model([input_ids,attention_masks])\n    output = output[1]\n    \n    output = tf.keras.layers.Dense(32, activation='relu')(output)\n    output = tf.keras.layers.Dropout(dropout)(output)\n\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs=[input_ids,attention_masks], outputs=output)\n    \n    model.compile(\n        Adam(learning_rate=learning_rate), \n        loss='binary_crossentropy', \n        metrics=['accuracy', f1, precision, recall]\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:16:56.977529Z","iopub.execute_input":"2022-12-02T04:16:56.977918Z","iopub.status.idle":"2022-12-02T04:16:56.985815Z","shell.execute_reply.started":"2022-12-02T04:16:56.97788Z","shell.execute_reply":"2022-12-02T04:16:56.984757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFRobertaModel\n\nrobertaoscar_pretrained_model = TFRobertaModel.from_pretrained(pretrained_roberta_oscar, from_pt=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:17:13.611546Z","iopub.execute_input":"2022-12-02T04:17:13.611931Z","iopub.status.idle":"2022-12-02T04:18:03.082259Z","shell.execute_reply.started":"2022-12-02T04:17:13.6119Z","shell.execute_reply":"2022-12-02T04:18:03.081316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"robertaoscar_model = robertaoscar(robertaoscar_pretrained_model, dropout=0.1, learning_rate=5e-5)\nrobertaoscar_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:20:16.316812Z","iopub.execute_input":"2022-12-02T04:20:16.317293Z","iopub.status.idle":"2022-12-02T04:20:17.896726Z","shell.execute_reply.started":"2022-12-02T04:20:16.317254Z","shell.execute_reply":"2022-12-02T04:20:17.895441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\npath = 'illustration'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n    \nplot_model(robertaoscar_model, to_file=f'{path}/Indonesian RoBERTa OSCAR.png', expand_nested=True, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:20:21.507106Z","iopub.execute_input":"2022-12-02T04:20:21.508208Z","iopub.status.idle":"2022-12-02T04:20:21.756406Z","shell.execute_reply.started":"2022-12-02T04:20:21.508135Z","shell.execute_reply":"2022-12-02T04:20:21.755107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(robertaoscar_pretrained_model)\ndel(robertaoscar_model)\nkeras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:20:24.62963Z","iopub.execute_input":"2022-12-02T04:20:24.630029Z","iopub.status.idle":"2022-12-02T04:20:24.720155Z","shell.execute_reply.started":"2022-12-02T04:20:24.629993Z","shell.execute_reply":"2022-12-02T04:20:24.719014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets = train_data['label'].values\nvalidation_targets = validation_data['label'].values\ntest_targets = test_data['label'].values","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:20:26.408115Z","iopub.execute_input":"2022-12-02T04:20:26.409242Z","iopub.status.idle":"2022-12-02T04:20:26.415342Z","shell.execute_reply.started":"2022-12-02T04:20:26.409201Z","shell.execute_reply":"2022-12-02T04:20:26.414047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learing_rate = [1e-5, 2e-5, 3e-5]\nbatch = [16, 32]\n\nbatch_for_test_result = []\nlearing_rate_for_test_result = []\ntest_loss = []\ntest_accuracy = []\ntest_f1 = []\ntest_precision = []\ntest_recall = []\n\nearly_stopping = EarlyStopping(monitor='val_f1', patience=3, verbose=0, mode='max')\n\nif not os.path.exists('results/models'):\n    os.makedirs('results/models')\n\nfor b in batch:\n    for lr in learing_rate:\n        print(f'Training RoBERTa OSCAR Model (Batch Size = {b}, Learning Rate = {lr})')\n\n        robertaoscar_pretrained_model = TFRobertaModel.from_pretrained(pretrained_roberta_oscar, from_pt=True)\n        robertaoscar_model = robertaoscar(robertaoscar_pretrained_model, dropout=0.1, learning_rate=lr)\n        \n        save_best = ModelCheckpoint(\n            f'results/models/robertaoscar_batch={b}_lr={lr}.hdf5', \n            save_best_only=True, \n            save_weights_only=True,\n            monitor='val_f1', \n            mode='max'\n        )\n\n        history = robertaoscar_model.fit(\n            [train_input_ids,train_attention_masks], \n            train_targets, \n            validation_data=([validation_input_ids, validation_attention_masks], validation_targets),\n            callbacks=[early_stopping, save_best],\n            epochs=4, \n            batch_size=b,\n            verbose=1\n        )\n\n        save_graph(history, lr, b, 'RoBERTa OSCAR')\n        save_history(history, lr, b, 'RoBERTa OSCAR')\n\n        print(f'Testing RoBERTa OSCAR Model (Batch Size = {b}, Learning Rate = {lr})')\n        \n        robertaoscar_model.load_weights(f'results/models/robertaoscar_batch={b}_lr={lr}.hdf5')\n        \n        evaluation = robertaoscar_model.evaluate(\n            [test_input_ids, test_attention_masks], \n            test_targets, \n            batch_size=b,\n            verbose=1\n        )\n        \n        batch_for_test_result.append(b)\n        learing_rate_for_test_result.append(lr)\n        test_loss.append(evaluation[0])\n        test_accuracy.append(evaluation[1])\n        test_f1.append(evaluation[2])\n        test_precision.append(evaluation[3])\n        test_recall.append(evaluation[4])\n\n        del(robertaoscar_pretrained_model)\n        del(robertaoscar_model)\n        del(history)\n        del(evaluation)\n        keras.backend.clear_session()\n\n\ntest_result_df = pd.DataFrame({\n    'Training Batch Size': batch_for_test_result,\n    'Training Learning Rate': learing_rate_for_test_result, \n    'Test Loss': test_loss, \n    'Test Accuracy': test_accuracy,\n    'Test F1-Score': test_f1,\n    'Test Precision': test_precision,\n    'Test Recall': test_recall,\n})\n\npath = f'results/evaluation'\n    \nif not os.path.exists(path):\n    os.makedirs(path)\n\ntest_result_df.to_csv(f'{path}/RoBERTa OSCAR.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T04:20:29.972619Z","iopub.execute_input":"2022-12-02T04:20:29.972992Z","iopub.status.idle":"2022-12-02T04:29:12.115847Z","shell.execute_reply.started":"2022-12-02T04:20:29.972959Z","shell.execute_reply":"2022-12-02T04:29:12.114874Z"},"trusted":true},"execution_count":null,"outputs":[]}]}